{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_folder = '<folder_with_generated_tables>'\n",
    "output_folder = '<folder_to_save_results>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(value, is_date=False):\n",
    "    if value != value:\n",
    "        return ''\n",
    "\n",
    "    if is_date:\n",
    "        try:\n",
    "            return date_parser.parse(value)\n",
    "        except:\n",
    "            try:\n",
    "                return pd.to_datetime(value)\n",
    "            except:\n",
    "                pass    \n",
    "        \n",
    "    if type(value).__module__ == 'numpy':\n",
    "        value = value.item()\n",
    "\n",
    "    if isinstance(value, str):            \n",
    "        if value.startswith('-') and value.replace('-', '').replace(',', '').isdigit():                \n",
    "            return int(value.replace(',', ''))\n",
    "        if value.replace(',', '').isdigit():                \n",
    "            return int(value.replace(',', ''))\n",
    "        if value.startswith('=') and value.isdigit():                \n",
    "            return int(value.replace('=', ''))\n",
    "        if value.startswith('-') and value.replace('-', '').replace(',', '').replace('.', '').isdigit():                \n",
    "            return float(value.replace(',', ''))\n",
    "        if value.replace(',', '').replace('.', '').isdigit():                \n",
    "            return float(value.replace(',', ''))\n",
    "        \n",
    "        value = value.strip().lower()\n",
    "\n",
    "        if value in ('none', 'n/a', 'nan', '-'):\n",
    "            return '' \n",
    "\n",
    "        value = value.replace('&', 'and')\n",
    "\n",
    "        if value == 'united states':\n",
    "            return 'usa'\n",
    "        if value == 'united kingdom':\n",
    "            return 'uk'\n",
    "        \n",
    "        value = unidecode(value)        \n",
    "        value = ''.join(c for c in value if c.isalnum()) \n",
    "        return value\n",
    "\n",
    "    return value\n",
    "\n",
    "def normalize_key(value, is_date=False):\n",
    "    if value != value:\n",
    "        return ''\n",
    "\n",
    "    if is_date:\n",
    "        try:\n",
    "            return str(date_parser.parse(value))\n",
    "        except:\n",
    "            try:\n",
    "                return str(pd.to_datetime(value))\n",
    "            except:\n",
    "                pass  \n",
    "\n",
    "    if isinstance(value, str):  \n",
    "        value = value.strip().lower()\n",
    "\n",
    "        if value in ('none', 'n/a', 'nan', '-', '--', 'unknown'):\n",
    "            return '' \n",
    "\n",
    "        value = value.replace('&', 'and')\n",
    "\n",
    "        if value == 'united states':\n",
    "            return 'usa'\n",
    "        if value == 'united kingdom':\n",
    "            return 'uk'\n",
    "\n",
    "        value = unidecode(value)        \n",
    "        value = ''.join(c for c in value if c.isalnum()) \n",
    "        return value\n",
    "\n",
    "    return str(value)\n",
    "\n",
    "def normalize_primary_columns(df, norm_columns, date_columns, primary_columns, keys_type):\n",
    "    for col in norm_columns:\n",
    "        df[col] = df[col].apply(normalize_key, col in date_columns)\n",
    "    \n",
    "    for col, key_type in zip(primary_columns, keys_type):\n",
    "        if key_type == 'year':\n",
    "            df[col] = df[col].astype(float).astype(int)\n",
    "            \n",
    "        df[col] = df[col].astype(str)\n",
    "        \n",
    "    return [tuple(r) for r in df[primary_columns].to_numpy()]    \n",
    "\n",
    "def find_row(df, columns, values):\n",
    "    query = ' & '.join([f'(`{col}`==\"{value}\")' for col, value in zip(columns, values)])    \n",
    "    return df.query(query)                    \n",
    "\n",
    "def evaluate_table(df_fetched, df_ref, primary_columns, keys_type, date_columns, epsilons):\n",
    "    columns = df_ref.columns\n",
    "    df_fetched.columns = columns    \n",
    "    df_fetched = df_fetched.drop_duplicates(subset=primary_columns)   \n",
    "\n",
    "    norm_columns = set(primary_columns)        \n",
    "    for pc in primary_columns:\n",
    "        df_fetched = df_fetched[df_fetched[pc].notna()]    \n",
    "    \n",
    "    fetched_entities = normalize_primary_columns(df_fetched, norm_columns, date_columns, primary_columns, keys_type)\n",
    "    ref_entities = normalize_primary_columns(df_ref, norm_columns, date_columns, primary_columns, keys_type)\n",
    "\n",
    "    total_matches = 0\n",
    "    key_matches = 0\n",
    "\n",
    "    for fetched_entity in fetched_entities:\n",
    "        if fetched_entity in ref_entities: \n",
    "            row_fetched = find_row(df_fetched, primary_columns, fetched_entity)\n",
    "            row_ref = find_row(df_ref, primary_columns, fetched_entity)\n",
    "            key_matches += 1\n",
    "            \n",
    "            for column in columns:\n",
    "                try:\n",
    "                    value_fetched = row_fetched[column].values[0]\n",
    "                    value_ref = row_ref[column].values[0]\n",
    "\n",
    "                    norm_value_fetched = normalize_value(value_fetched, column in date_columns)\n",
    "                    norm_value_ref = normalize_value(value_ref, column in date_columns)\n",
    "\n",
    "                    if norm_value_fetched == norm_value_ref:    \n",
    "                        total_matches += 1\n",
    "                    elif column in epsilons and norm_value_ref != '' and norm_value_fetched != '':\n",
    "                        if norm_value_ref * 0.999 < norm_value_fetched < norm_value_ref * 1.001:\n",
    "                            total_matches += 1   \n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "    recall = total_matches/(df_ref.shape[0] * df_ref.shape[1])\n",
    "    precision = total_matches/(df_fetched.shape[0] * df_fetched.shape[1])\n",
    "    f1_score = 2*recall*precision/(recall+precision) if (recall + precision) > 0 else 0.0\n",
    "\n",
    "    keys_recall = key_matches/len(ref_entities)\n",
    "    keys_precision = key_matches/len(fetched_entities)\n",
    "    keys_f1_score = 2*keys_recall*keys_precision/(keys_recall+keys_precision) if (keys_recall + keys_precision) > 0 else 0.0\n",
    "    \n",
    "    nk = len(primary_columns)\n",
    "    \n",
    "    non_keys_recall = (total_matches - key_matches*nk) / (df_ref.shape[0] * (df_ref.shape[1] - nk))\n",
    "    non_keys_precision = (total_matches - key_matches*nk) / (df_fetched.shape[0] * (df_fetched.shape[1] - nk))\n",
    "    non_keys_f1_score = 2*non_keys_recall*non_keys_precision/(non_keys_recall+non_keys_precision) if (non_keys_recall + non_keys_precision) > 0 else 0.0\n",
    "     \n",
    "    relative_non_key_accuracy = (total_matches - key_matches*nk) / (key_matches * (df_ref.shape[1] - nk))    \n",
    "        \n",
    "    return keys_recall, keys_precision, keys_f1_score, non_keys_recall, non_keys_precision, non_keys_f1_score, recall, precision, f1_score, relative_non_key_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path=\"../benchmark/cfg.json\"\n",
    "\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "\n",
    "keys_recall_scores = []\n",
    "keys_precision_scores = []\n",
    "keys_f1_scores = []\n",
    "non_keys_recall_scores = []\n",
    "non_keys_precision_scores = []\n",
    "non_keys_f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "rel_nk_acc_scores = []\n",
    "\n",
    "for i in range(100):\n",
    "    idx = \"%d\" % i\n",
    "    md = metadata[idx]\n",
    "    print(md['name'])\n",
    "    try:\n",
    "        fetched_table = pd.read_csv(os.path.join(tables_folder, \"%s.csv\" % md['name']))\n",
    "        gt_table = pd.read_csv(md['path'])\n",
    "        primary_columns = md['keys']\n",
    "        keys_type = md['keys_type']\n",
    "        date_columns = md['dateColumns']\n",
    "        epsilons = md['epsilons']\n",
    "        kr, kp, kf1, nkr, nkp, nkf1, r, p, f1, rnka  = evaluate_table(fetched_table, \n",
    "                                                                       gt_table, \n",
    "                                                                       primary_columns, \n",
    "                                                                       keys_type,\n",
    "                                                                       date_columns, \n",
    "                                                                       epsilons)\n",
    "    except:    \n",
    "        kr, kp, kf1, nkr, nkp, nkf1, r, p, f1, rnka = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "        \n",
    "    tables.append(md['name'])\n",
    "    \n",
    "    keys_recall_scores.append(kr)\n",
    "    keys_precision_scores.append(kp)\n",
    "    keys_f1_scores.append(kf1)\n",
    "    non_keys_recall_scores.append(nkr)\n",
    "    non_keys_precision_scores.append(nkp)\n",
    "    non_keys_f1_scores.append(nkf1)\n",
    "    recall_scores.append(r)\n",
    "    precision_scores.append(p)\n",
    "    f1_scores.append(f1)\n",
    "    rel_nk_acc_scores.append(rnka)\n",
    "    \n",
    "    print(\"====================\")\n",
    "    print(\"Keys Recall: %.4f\" % kr)\n",
    "    print(\"Keys Precision: %.4f\" % kp)\n",
    "    print(\"Keys F1: %.4f\" % kf1)\n",
    "    \n",
    "    print(\"Non Keys Recall: %.4f\" % nkr)\n",
    "    print(\"Non Keys Precision: %.4f\" % nkp)\n",
    "    print(\"Non Keys F1: %.4f\" % nkf1)\n",
    "    \n",
    "    print(\"Relative Non Keys Accuracy: %.4f\" % rnka)\n",
    "    \n",
    "    print(\"Recall: %.4f\" % r)\n",
    "    print(\"Precision: %.4f\" % p)\n",
    "    print(\"F1: %.4f\" % f1)    \n",
    "    \n",
    "    print(\"====================\")    \n",
    "    \n",
    "res_df = pd.DataFrame([tables, \n",
    "                       keys_recall_scores,\n",
    "                       keys_precision_scores,\n",
    "                       keys_f1_scores,\n",
    "                       non_keys_recall_scores,\n",
    "                       non_keys_precision_scores,\n",
    "                       non_keys_f1_scores,\n",
    "                       rel_nk_acc_scores,\n",
    "                       recall_scores,\n",
    "                       precision_scores, \n",
    "                       f1_scores                       \n",
    "                       ]).T\n",
    "res_df.columns = ['Table', \n",
    "                  'Keys_Recall', 'Keys_Precision', 'Keys_F1_Score',\n",
    "                  'Non_Keys_Recall', 'Non_Keys_Precision', 'Non_Keys_F1_Score', 'Rel_Non_Keys_Accuracy',\n",
    "                  'Recall', 'Precision', 'F1_Score'\n",
    "                  ]\n",
    "\n",
    "res_df['Keys_Recall'] = res_df['Keys_Recall'].astype(float).round(4)\n",
    "res_df['Keys_Precision'] = res_df['Keys_Precision'].astype(float).round(4)\n",
    "res_df['Keys_F1_Score'] = res_df['Keys_F1_Score'].astype(float).round(4)\n",
    "res_df['Non_Keys_Recall'] = res_df['Non_Keys_Recall'].astype(float).round(4)\n",
    "res_df['Non_Keys_Precision'] = res_df['Non_Keys_Precision'].astype(float).round(4)\n",
    "res_df['Non_Keys_F1_Score'] = res_df['Non_Keys_F1_Score'].astype(float).round(4)\n",
    "res_df['Rel_Non_Keys_Accuracy'] = res_df['Rel_Non_Keys_Accuracy'].astype(float).round(4)\n",
    "res_df['Recall'] = res_df['Recall'].astype(float).round(4)\n",
    "res_df['Precision'] = res_df['Precision'].astype(float).round(4)\n",
    "res_df['F1_Score'] = res_df['F1_Score'].astype(float).round(4)\n",
    "\n",
    "means = pd.DataFrame(['All'] + res_df.mean(axis=0, numeric_only=True).tolist()).T\n",
    "means.columns = res_df.columns        \n",
    "res_df = pd.concat([res_df, means], axis=0)\n",
    "\n",
    "res_df.to_csv(\"%s/scores.csv\" % result_folder, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
