{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from dateutil import parser as date_parser\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingError(Exception):\n",
    "    pass\n",
    "\n",
    "class WrongKeyError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableGenerator_JSON():\n",
    "    SYSTEM_MSG = \"You are a retriever of facts.\"\n",
    "    \n",
    "    CELL_TEMPLATE = \"\"\"\n",
    "    We want to create a table with the detailed information about %s.\n",
    "    Columns in the table are %s.\n",
    "    %s.  \n",
    "    For the table row whose key is %s what is the value of attribute %s.\n",
    "    The response will be formatted as JSON dictionary shown below.\n",
    "    Pay special attention to wrap all property names and values in double quotes!\n",
    "    \n",
    "    RESPONSE FORMAT:\n",
    "    {\n",
    "        %s\n",
    "    }\n",
    "    \"\"\" \n",
    "    \n",
    "    def _norm_field(self, s):\n",
    "        s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "                .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "                .replace(\"/\", \"\")\n",
    "        return re.sub('_+', '_', s)\n",
    "    \n",
    "    def _key_columns(self, keys):\n",
    "        if len(keys) == 1:\n",
    "            return \"The key column in the table is %s\" % keys[0]\n",
    "        else:\n",
    "            return \"The key columns in the table are %s\" % \", \".join(keys)\n",
    "    \n",
    "    def parse_keys_response(self, response, keys): \n",
    "        try:\n",
    "            if not response.startswith(\"[\") and \"[\" in response:\n",
    "                response = response[response.find(\"[\"):]\n",
    "\n",
    "            if not response.endswith(\"]\") and \"]\" in response:\n",
    "                response = response[:response.rfind(\"]\")+1]\n",
    "\n",
    "            if '[' not in response and ']' not in response and '{' in response and '}' in response:\n",
    "                response = '[' + response + ']'    \n",
    "\n",
    "            response_json = json.loads(response)\n",
    "\n",
    "            if isinstance(response_json, dict) and len(response_json.keys()) == 1:\n",
    "                response_json = list(response_json.values())[0]    \n",
    "        except:  \n",
    "            split_response = response.split(\"{\")\n",
    "            response_json = []\n",
    "            for s in split_response[1:]:\n",
    "                split_s = s.split(\"}\")\n",
    "                if len(split_s) > 1:\n",
    "                    content = split_s[0]\n",
    "                    attributes = content.split(\",\")\n",
    "                    elements = {}\n",
    "                    for attr in attributes:\n",
    "                        knv = attr.split(\":\")   \n",
    "                        if len(knv) > 1:\n",
    "                            parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                            parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                            elements[parsed_k] = parsed_v\n",
    "\n",
    "                    response_json.append(elements)\n",
    "        \n",
    "        norm_keys = [self._norm_field(key) for key in keys]\n",
    "        keys_json = []\n",
    "        for item in response_json:\n",
    "            key_item = {}\n",
    "            for key in norm_keys:\n",
    "                key_item[key] = item[key]\n",
    "            keys_json.append(key_item)    \n",
    "        \n",
    "        return keys_json\n",
    "    \n",
    "    def generate_cell_prompt(self, query, keys, fields, field, fetched_key):\n",
    "        keys = [self._norm_field(k) for k in keys]\n",
    "        key_columns = self._key_columns(keys)    \n",
    "        \n",
    "        fields = [self._norm_field(f) for f in fields]\n",
    "        all_columns = \", \".join(fields)\n",
    "        \n",
    "        keys = [self._norm_field(k) for k in keys]\n",
    "        key_json = []   \n",
    "        for key in keys:\n",
    "            key_value = fetched_key[key]\n",
    "            key_json.append(\"%s = %s\" % (key, key_value))\n",
    "\n",
    "        row_key = '(%s)' % ', '.join(key_json)\n",
    "        \n",
    "        field = self._norm_field(field)\n",
    "        response_format = '{\"%s\": \"value of %s\"}' % (field, field)\n",
    "        \n",
    "        prompt = self.CELL_TEMPLATE % (query, all_columns, key_columns, row_key, field, response_format)    \n",
    "        return prompt    \n",
    "    \n",
    "    def parse_cell_response(self, response): \n",
    "        cell_content = response[response.find(\"{\"):response.rfind(\"}\")+1]\n",
    "        try:\n",
    "            return json.loads(cell_content)\n",
    "        except:\n",
    "            cell_content = cell_content[1:-1]\n",
    "            knv = cell_content.split(\":\") \n",
    "            if len(knv) == 2:\n",
    "                parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                return {parsed_k : parsed_v}\n",
    "            else:\n",
    "                raise ParsingError()\n",
    "    \n",
    "    def create_dataframe(self, rows, columns, keys, df_ref): \n",
    "        df = pd.DataFrame.from_dict(rows)  \n",
    "        columns = [self._norm_field(col) for col in columns]\n",
    "        df = df[columns]\n",
    "        df.columns = df_ref.columns\n",
    "        df = df.drop_duplicates(subset=keys)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner():\n",
    "    openai.api_key = \"\"\n",
    "    MODEL = \"gpt-4o\"\n",
    "    NOTE = 'generic_keys_cells'\n",
    "    \n",
    "    def __init__(self, table_generator, metadata_path, rows_result):\n",
    "        with open(metadata_path, \"rb\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        self.rows_result = rows_result    \n",
    "            \n",
    "        self.table_generator = table_generator\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        self.result_folder = \"DATA/%s_%s_%s\" % (self.MODEL.split(\"/\")[-1].replace('-', '_'), \n",
    "                                                   self.NOTE,\n",
    "                                                   time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        \n",
    "        print(\"Experiment result folder: %s\" % self.result_folder)\n",
    "        \n",
    "        os.makedirs(self.result_folder)\n",
    "        os.makedirs(\"%s/tables\" % self.result_folder)\n",
    "        \n",
    "        self.result = {}\n",
    "        \n",
    "    def normalize_key(self, value, is_date=False):\n",
    "        if value != value:\n",
    "            return ''\n",
    "\n",
    "        if is_date:\n",
    "            try:\n",
    "                return str(date_parser.parse(value))\n",
    "            except:\n",
    "                try:\n",
    "                    return str(pd.to_datetime(value))\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "        if isinstance(value, str):  \n",
    "            value = value.lower()\n",
    "\n",
    "            if value in ('none', 'n/a', 'nan', '-'):\n",
    "                return '' \n",
    "\n",
    "            value = value.replace('&', 'and')\n",
    "\n",
    "            if value == 'united states':\n",
    "                return 'usa'\n",
    "            if value == 'united kingdom':\n",
    "                return 'uk'\n",
    "\n",
    "            value = unidecode(value)        \n",
    "            value = ''.join(c for c in value if c.isalnum()) \n",
    "            return value\n",
    "\n",
    "        return str(value)\n",
    "\n",
    "    def normalize_primary_columns(self, df, norm_columns, date_columns, primary_columns):\n",
    "        for col in norm_columns:\n",
    "            df[col] = df[col].apply(self.normalize_key, col in date_columns)  \n",
    "        return [tuple(r) for r in df[primary_columns].to_numpy()]    \n",
    "        \n",
    "    def fetch_data(self, idx):\n",
    "        task = self.metadata[idx]\n",
    "        \n",
    "        task_name = task['name']        \n",
    "        print(\"Fetching data for %s\" % task_name)\n",
    "        \n",
    "        query = task['table_title']\n",
    "        keys = task['keys']\n",
    "        columns = task['columns'] \n",
    "        date_columns = task['dateColumns']    \n",
    "            \n",
    "        try:\n",
    "            keys_response = self.rows_result[idx]['keys_response'][0].strip()\n",
    "            self.result[idx] = {'keys_response': keys_response} \n",
    "            \n",
    "            parsed_keys_response = self.table_generator.parse_keys_response(keys_response, keys)\n",
    "            \n",
    "            print(\"Fetched %d key instances\" % len(parsed_keys_response))                       \n",
    "            \n",
    "            self.result[idx]['cell_prompts'] = []\n",
    "            self.result[idx]['cell_responses'] = []\n",
    "            rows = []            \n",
    "            \n",
    "            df_ref = pd.read_csv(task['path'])\n",
    "            ref_entities = self.normalize_primary_columns(df_ref, columns, date_columns, keys)\n",
    "            \n",
    "            norm_keys = [self.table_generator._norm_field(k) for k in keys]\n",
    "            norm_date_cols = [self.table_generator._norm_field(c) for c in date_columns]\n",
    "            \n",
    "            keys_already_checked = set()\n",
    "\n",
    "            for key_instance in parsed_keys_response:\n",
    "                keys_tuple = []\n",
    "                for nk in norm_keys:\n",
    "                    keys_tuple.append(self.normalize_key(key_instance[nk], nk in norm_date_cols))\n",
    "                keys_tuple = tuple(keys_tuple)\n",
    "\n",
    "                if keys_tuple in keys_already_checked:\n",
    "                    continue\n",
    "                keys_already_checked.add(keys_tuple)\n",
    "                \n",
    "                row = key_instance.copy()                  \n",
    "                for col in columns:\n",
    "                    if col in keys:\n",
    "                        continue\n",
    "                    try:\n",
    "                        if not keys_tuple in ref_entities:\n",
    "                            raise WrongKeyError()                            \n",
    "            \n",
    "                        cell_prompt_i = self.table_generator.generate_cell_prompt(query, keys, columns, col, key_instance)   \n",
    "                        self.result[idx]['cell_prompts'].append(cell_prompt_i)\n",
    "\n",
    "                        result = openai.ChatCompletion.create(\n",
    "                            model=self.MODEL,\n",
    "                            messages=[{\"role\": \"system\", \"content\": self.table_generator.SYSTEM_MSG},\n",
    "                            {\"role\": \"user\", \"content\": cell_prompt_i}],\n",
    "                            temperature=0)\n",
    "                    \n",
    "                        cell_response = result['choices'][0][\"message\"][\"content\"].strip()   \n",
    "                        self.result[idx]['cell_responses'].append(cell_response)\n",
    "\n",
    "                        parsed_cell_response = self.table_generator.parse_cell_response(cell_response)\n",
    "                        row.update(parsed_cell_response)\n",
    "                    except Exception as ie:\n",
    "                        print(ie.__class__.__name__)\n",
    "                        field = self.table_generator._norm_field(col)\n",
    "                        failed_cell = '{\"%s\": \"%s\"}' % (field, ie.__class__.__name__)\n",
    "                        row.update(json.loads(failed_cell))\n",
    "                rows.append(row)                     \n",
    "        \n",
    "            df = self.table_generator.create_dataframe(rows, columns, keys, df_ref) \n",
    "\n",
    "            table_path = \"%s/tables/%s.csv\" % (self.result_folder, task_name)\n",
    "            self.result[idx]['table_path'] = table_path                \n",
    "            df.to_csv(table_path, index=False)            \n",
    "\n",
    "            print(\"Created table with %d rows\" % len(df))\n",
    "\n",
    "            return df\n",
    "        except Exception as e:  \n",
    "            print(e.__class__.__name__)\n",
    "            \n",
    "    def save_result(self):\n",
    "        with open(\"%s/result.json\" % self.result_folder, \"w\") as outfile:\n",
    "            result_json = json.dumps(self.result, indent=4)\n",
    "            outfile.write(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbr_results_path = 'DATA/gpt_4o_generic_keys_rows_20240728-073304'\n",
    "\n",
    "with open(os.path.join(rbr_results_path, 'result.json')) as f:\n",
    "    rows_result = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment result folder: DATA/gpt_4o_generic_keys_cells_20240926-153047\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 14\n",
      "Fetching data for figure_skating_ladies_2009_2010\n",
      "Fetched 10 key instances\n",
      "WrongKeyError\n",
      "WrongKeyError\n",
      "WrongKeyError\n",
      "WrongKeyError\n",
      "WrongKeyError\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 19\n",
      "Fetching data for oregon_crime_1960_2009\n",
      "Fetched 50 key instances\n",
      "Created table with 50 rows\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tg = TableGenerator_JSON()\n",
    "\n",
    "runner = ExperimentRunner(tg, \"DATA/benchmark/cfg.json\", rows_result)\n",
    "\n",
    "print(\"\\n====================\\n\")\n",
    "\n",
    "for i in [13,18]:\n",
    "    print(\"Table # %d\" % (i+1))\n",
    "    idx = \"%d\" % i\n",
    "    table = runner.fetch_data(idx)\n",
    "    print(\"\\n====================\\n\")\n",
    "    \n",
    "runner.save_result()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
