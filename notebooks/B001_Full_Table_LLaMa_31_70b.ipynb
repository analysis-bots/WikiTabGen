{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from dateutil import parser as date_parser\n",
    "from unidecode import unidecode\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableGenerator_JSON():\n",
    "    TEMPLATE = \"\"\"\n",
    "    [INST] <<SYS>>\n",
    "    You are a retriever of facts.\n",
    "    <</SYS>>  \n",
    "    \n",
    "    List %s - as many as possible to fit into response.\n",
    "    The response will be formatted as JSON shown below.\n",
    "    Each element of the response will contain %d fields: %s.\n",
    "    Do not output any additional text that is not in JSON format.\n",
    "    \n",
    "    RESPONSE FORMAT:\n",
    "    [{\n",
    "        %s\n",
    "    }]\n",
    "\n",
    "    [/INST]\n",
    "    \"\"\"  \n",
    "    \n",
    "    def _norm_field(self, s):\n",
    "        s = s.lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\".\", \"\").replace(\",\",\"_\")\\\n",
    "                .replace(\"(\", \"\").replace(\")\", \"\").replace(\":\", \"\").replace('\"','').replace(\"'\",\"\")\\\n",
    "                .replace(\"/\", \"\")\n",
    "        return re.sub('_+', '_', s)\n",
    "        \n",
    "    def generate_prompt(self, query, fields):\n",
    "        num_fields = len(fields)\n",
    "        fields_json = []\n",
    "        fields = [self._norm_field(f) for f in fields]\n",
    "        for field in fields:\n",
    "            fields_json.append('\"%s\": \"%s\"' % ('_'.join(field.replace(\"-\", \" \").split()), field))\n",
    "        response_format = ', '.join(fields_json)\n",
    "        prompt = self.TEMPLATE % (query, num_fields, fields, response_format)\n",
    "        return prompt       \n",
    "    \n",
    "    def parse_llm_response(self, response): \n",
    "        res = []\n",
    "        try:\n",
    "            if not response.startswith(\"[\") and \"[\" in response:\n",
    "                response = response[response.find(\"[\"):]\n",
    "\n",
    "            if not response.endswith(\"]\") and \"]\" in response:\n",
    "                response = response[:response.rfind(\"]\")+1]\n",
    "\n",
    "            if '[' not in response and ']' not in response and '{' in response and '}' in response:\n",
    "                response = '[' + response + ']'    \n",
    "\n",
    "            response_json = json.loads(response)\n",
    "\n",
    "            if isinstance(response_json, dict) and len(response_json.keys()) == 1:\n",
    "                response_json = list(response_json.values())[0]    \n",
    "        except:  \n",
    "            split_response = response.split(\"{\")\n",
    "            response_json = []\n",
    "            for s in split_response[1:]:\n",
    "                split_s = s.split(\"}\")\n",
    "                if len(split_s) > 1:\n",
    "                    content = split_s[0]\n",
    "                    attributes = content.split(\",\")\n",
    "                    elements = {}\n",
    "                    for attr in attributes:\n",
    "                        knv = attr.split(\":\")   \n",
    "                        if len(knv) > 1:\n",
    "                            parsed_k = \"%s\" % knv[0].replace('\"','').strip()\n",
    "                            parsed_v = \"%s\" % knv[1].replace('\"','').strip()\n",
    "                            elements[parsed_k] = parsed_v\n",
    "\n",
    "                    response_json.append(elements)  \n",
    "\n",
    "        df = pd.DataFrame.from_records(response_json) \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner():\n",
    "    openai.api_key = \"\"\n",
    "    openai.api_base = \"https://api.deepinfra.com/v1/openai\"\n",
    "    MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "    NOTE = 'full_table'\n",
    "    MAX_LEN = 3900\n",
    "    \n",
    "    def __init__(self, table_generator, metadata_path):\n",
    "        with open(metadata_path, \"rb\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "            \n",
    "        self.table_generator = table_generator\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        \n",
    "        self.result_folder = \"DATA/%s_%s_%s\" % (self.MODEL.split(\"/\")[-1].replace('-', '_'), \n",
    "                                                   self.NOTE,\n",
    "                                                   time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        \n",
    "        print(\"Experiment result folder: %s\" % self.result_folder)\n",
    "        \n",
    "        os.makedirs(self.result_folder)\n",
    "        os.makedirs(\"%s/Tables\" % self.result_folder)\n",
    "        \n",
    "        self.result = {}\n",
    "        \n",
    "    def fetch_data(self, idx):\n",
    "        task = self.metadata[idx]\n",
    "        \n",
    "        task_name = task['name']        \n",
    "        print(\"Fetching data for %s\" % task_name)\n",
    "        \n",
    "        query, columns = task['table_title'], task['columns']            \n",
    "        print(\"Query: \", query) \n",
    "        \n",
    "        prompt = self.table_generator.generate_prompt(query, columns)        \n",
    "\n",
    "        self.result[idx] = {'prompt': prompt}\n",
    "        \n",
    "        try:\n",
    "            max_tokens = self.MAX_LEN - len(self.encoding.encode(prompt))\n",
    "            result = openai.ChatCompletion.create(\n",
    "                model=self.MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "   \n",
    "            response = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "            if 'response' in self.result[idx]:\n",
    "                self.result[idx]['response'].append(response)\n",
    "            else:    \n",
    "                self.result[idx]['response'] = [response]\n",
    "\n",
    "            df = self.table_generator.parse_llm_response(response)          \n",
    "            df_ref = pd.read_csv(task['path'])          \n",
    "            df.columns = df_ref.columns\n",
    "            df = df.drop_duplicates(subset=task['keys'])\n",
    "\n",
    "            table_path = \"%s/Tables/%s.csv\" % (self.result_folder, task_name)\n",
    "            self.result[idx]['table_path'] = table_path                \n",
    "            df.to_csv(table_path, index=False)\n",
    "\n",
    "            print(\"Created table with %d rows\" % len(df))\n",
    "\n",
    "            return df\n",
    "        except Exception as e:              \n",
    "            print(e.__class__.__name__)\n",
    "            print(e)\n",
    "    \n",
    "    def save_result(self):\n",
    "        with open(\"%s/result.json\" % self.result_folder, \"w\") as outfile:\n",
    "            result_json = json.dumps(self.result, indent=4)\n",
    "            outfile.write(result_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment result folder: DATA/Meta_Llama_3.1_70B_Instruct_full_table_20240726-231535\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 1\n",
      "Fetching data for republican_straw_polls_2012\n",
      "Query:  results of straw polls for the Republican Party presidential primaries, 2012\n",
      "Created table with 32 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 2\n",
      "Fetching data for russia_demographics_1946_2012\n",
      "Query:  vital statistics of Russia's demographics from 1946 to 2012\n",
      "Created table with 15 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 3\n",
      "Fetching data for belgium_demographics_1900_2011\n",
      "Query:  vital statistics of Belgium's demographics from 1900 to 2011\n",
      "Created table with 13 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 4\n",
      "Fetching data for australia_demographics_1900_2010\n",
      "Query:  vital statistics of Australia's demographics from 1900 to 2010\n",
      "Created table with 23 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 5\n",
      "Fetching data for new_brunswick_parishes_2006_2011\n",
      "Query:  population statistics of the parishes in New Brunswick\n",
      "Created table with 56 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 6\n",
      "Fetching data for ice_hockey_2006\n",
      "Query:  statistics for men's ice hockey at the 2006 Winter Olympics\n",
      "Created table with 77 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 7\n",
      "Fetching data for biathlon_sprint_standings_2009_10\n",
      "Query:  results of men's sprint in 2009-10 Biathlon World Cup\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 8\n",
      "Fetching data for anaheim_ducks_draft_picks_1998_2013\n",
      "Query:  Anaheim Ducks draft picks from 1998 to 2013\n",
      "Created table with 65 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 9\n",
      "Fetching data for south_african_class_15f_4_8_2\n",
      "Query:  South African Class 15F 4-8-2 steam locomotives from 1938 to 1946\n",
      "Created table with 120 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 10\n",
      "Fetching data for tour_de_france_2009\n",
      "Query:  riders who took part in 2009 Tour de France\n",
      "Created table with 110 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 11\n",
      "Fetching data for men_butterfly_100m_2009\n",
      "Query:  men's 100 metre butterfly results in heats at the 2009 World Aquatics Championships\n",
      "Created table with 72 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 12\n",
      "Fetching data for playstation_3_cooperative_games\n",
      "Query:  cooperative games for the PlayStation 3\n",
      "Created table with 39 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 13\n",
      "Fetching data for rock_band_downloadable_2011\n",
      "Query:  downloadable songs for the Rock Band series released in 2011\n",
      "Created table with 57 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 14\n",
      "Fetching data for figure_skating_ladies_2009_2010\n",
      "Query:  best scores for ladies in 2009-10 figure skating season\n",
      "Created table with 6 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 15\n",
      "Fetching data for minor_planets_discovered_by_nikolai_chernykh\n",
      "Query:  minor planets discovered by Nikolai Chernykh\n",
      "Created table with 164 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 16\n",
      "Fetching data for elements\n",
      "Query:  chemical elements\n",
      "Created table with 38 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 17\n",
      "Fetching data for curling_teams_women_2013_2014\n",
      "Query:  teams on the women's 2013-14 World Curling Tour\n",
      "Created table with 58 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 18\n",
      "Fetching data for scottish_football_transfers_summer_2011\n",
      "Query:  Scottish football transfers in summer 2011\n",
      "Created table with 70 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 19\n",
      "Fetching data for oregon_crime_1960_2009\n",
      "Query:  crime statistics in Oregon from 1960 to 2009\n",
      "Created table with 36 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 20\n",
      "Fetching data for living_proof_the_farewell_tour\n",
      "Query:  concerts of Living Proof: The Farewell Tour\n",
      "Created table with 109 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 21\n",
      "Fetching data for new_zealand_demographics_1921_2011\n",
      "Query:  vital statistics of New Zealand's demographics from 1921 to 2011\n",
      "Created table with 19 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 22\n",
      "Fetching data for liechtenstein_demographics_1901_2011\n",
      "Query:  vital statistics of Liechtenstein's demographics from 1901 to 2011\n",
      "Created table with 12 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 23\n",
      "Fetching data for usa_demographics_1935_2010\n",
      "Query:  vital statistics of USA demographics from 1935 to 2010\n",
      "Created table with 16 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 24\n",
      "Fetching data for andorra_demographics_1948_2012\n",
      "Query:  vital statistics of Andorra's demographics from 1948 to 2012\n",
      "Created table with 15 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 25\n",
      "Fetching data for iran_pro_league_2012_2013\n",
      "Query:  team positions by round in Iran Pro League 2012-13\n",
      "Created table with 15 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 26\n",
      "Fetching data for english_latin_rivalry_1887_2012\n",
      "Query:  results of school football rivalry between Boston Latin School and English High School of Boston from 1887 to 2012\n",
      "Created table with 126 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 27\n",
      "Fetching data for formula_one_drivers_2010\n",
      "Query:  driver statistics from 2010 Formula One season\n",
      "Created table with 25 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 28\n",
      "Fetching data for sadaharu_oh_1959_1980\n",
      "Query:  statistics of Sadaharu Oh from 1959 to 1980\n",
      "Created table with 22 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 29\n",
      "Fetching data for equestrian_2012\n",
      "Query:  results of equestrian individual eventing at the 2012 Summer Olympics\n",
      "Created table with 50 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 30\n",
      "Fetching data for tennessee_vanderbilt_rivalry_1900_2012\n",
      "Query:  results of Tennessee-Vanderbilt football rivalry from 1900 to 2012\n",
      "Created table with 92 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 31\n",
      "Fetching data for classic_100_ten_years_on\n",
      "Query:  countdown results of Classic 100 Ten Years On (ABC)\n",
      "Created table with 49 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 32\n",
      "Fetching data for udaykumar_films\n",
      "Query:  Udaykumar's films from 1956 to 1985\n",
      "Created table with 53 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 33\n",
      "Fetching data for fa_cup_qualifying_rounds_1999_2000\n",
      "Query:  results of 1999-2000 FA Cup qualifying rounds\n",
      "Created table with 102 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 34\n",
      "Fetching data for portuguese_grape_varieties\n",
      "Query:  Portuguese grape varieties\n",
      "Created table with 22 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 35\n",
      "Fetching data for ramsar_convention_parties\n",
      "Query:  parties to the Ramsar Convention from 1 January 1975 to 31 December 2012\n",
      "Created table with 136 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 36\n",
      "Fetching data for guitar_hero_5_songs\n",
      "Query:  songs in Guitar Hero 5\n",
      "Created table with 65 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 37\n",
      "Fetching data for south_cambridgeshire_district_council_1973_2012\n",
      "Query:  composition figures of the South Cambridgeshire District Council from 1973 to 2012\n",
      "Created table with 16 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 38\n",
      "Fetching data for dublin_maternity_hospital_mortality_rates_1784_1849\n",
      "Query:  yearly patient mortality rates at the Dublin Maternity Hospital from 1784 to 1849\n",
      "Created table with 66 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 39\n",
      "Fetching data for ship_launches_january_1944\n",
      "Query:  ships launched in January 1944\n",
      "Created table with 26 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 40\n",
      "Fetching data for uk_demographics_1960_2012\n",
      "Query:  vital statistics of UK demographics from 1960 to 2012\n",
      "Created table with 34 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 41\n",
      "Fetching data for fukushima_plant_operating_history_1970_2009\n",
      "Query:  operating history of Fukushima Daiichi Nuclear Power Plant from 1970 to 2009\n",
      "Created table with 40 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 42\n",
      "Fetching data for new_zealand_football_results_1922_2012\n",
      "Query:  New Zealand national football team's cumulative results from 1922 to 2012\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 43\n",
      "Fetching data for jack_nicklaus_achievements_1962_2005\n",
      "Query:  career achievements by Jack Nicklaus from 1962 to 2005\n",
      "Created table with 44 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 44\n",
      "Fetching data for bond_credit_rating_1981_2008\n",
      "Query:  Standard & Poor's one-year global corporate default rates from 1981 to 2008\n",
      "Created table with 25 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 45\n",
      "Fetching data for baltimore_oreoles_2012\n",
      "Query:  pitching stats for 2012 Baltimore Orioles season\n",
      "Created table with 17 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 46\n",
      "Fetching data for david_robinson_40_pts_games\n",
      "Query:  stats for games in which David Robinson scored 40 or more points\n",
      "Created table with 18 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 47\n",
      "Fetching data for latin_american_migration_to_uk_1997_2008\n",
      "Query:  statistics on Latin American migration to the United Kingdom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table with 19 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 48\n",
      "Fetching data for european_countries_gdp_2007_2012\n",
      "Query:  sovereign states in Europe by GDP (nominal)\n",
      "Created table with 48 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 49\n",
      "Fetching data for royal_dulton_figurines_HN4100_HN4199\n",
      "Query:  Royal Dulton figurines from HN4100 to HN4199\n",
      "Created table with 83 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 50\n",
      "Fetching data for adaalat_episodes_2012\n",
      "Query:  Adaalat episodes from 1 January 2012 to 31 December 2012\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 51\n",
      "Fetching data for viktoria_plzen_1993_2012\n",
      "Query:  stats of FC Viktoria Plzen from 1993-1994 to 2012-2013\n",
      "Created table with 20 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 52\n",
      "Fetching data for just_dance_kids_2_tracks\n",
      "Query:  tracks of Just Dance Kids 2\n",
      "Created table with 34 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 53\n",
      "Fetching data for cross_country_junior_women_1996\n",
      "Query:  results of 1996 IAAF World Cross Country Championships in Junior women's race\n",
      "Created table with 20 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 54\n",
      "Fetching data for metropolitan_opera_us_premieres\n",
      "Query:  United States premieres at the Metropolitan Opera from 1883-12-20 to 2005-05-13\n",
      "Created table with 46 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 55\n",
      "Fetching data for kasparov_kramnik_1993_2004\n",
      "Query:  chess games between Kasparov and Kramnik from 1993 to 2004\n",
      "Created table with 24 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 56\n",
      "Fetching data for decathlon_top50_1999\n",
      "Query:  top 50 decathlon year rankings in 1999\n",
      "Created table with 50 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 57\n",
      "Fetching data for minor_planets_152601_152700\n",
      "Query:  minor planets in the name range 152601-152700 from 152601 to 152700\n",
      "Created table with 72 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 58\n",
      "Fetching data for two_and_a_half_men_season_7\n",
      "Query:  episodes of Two and a Half Men (season 7)\n",
      "Created table with 22 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 59\n",
      "Fetching data for moesha_season_7\n",
      "Query:  Moesha episodes, season 3\n",
      "Created table with 22 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 60\n",
      "Fetching data for miss_new_york_usa_delegates_2012\n",
      "Query:  delegates of Miss New York USA 2012\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 61\n",
      "Fetching data for clint_dolezel_stats_1995_2008\n",
      "Query:  career stats of Clint Dolezel from 1995 to 2008\n",
      "Created table with 14 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 62\n",
      "Fetching data for oxford_university_undergraduate_admissions_1988_2010\n",
      "Query:  University of Oxford undergraduate admissions statistics from 1988 to 2010\n",
      "Created table with 23 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 63\n",
      "Fetching data for serbia_demographics_1900_1912\n",
      "Query:  vital statistics of Serbia's demographics from 1900 to 1912\n",
      "Created table with 13 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 64\n",
      "Fetching data for protein_data_bank_1976_2012\n",
      "Query:  numbers of proteins added to Protein Data Bank from 1976 to 2012\n",
      "Created table with 37 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 65\n",
      "Fetching data for european_athletics_championships_1986\n",
      "Query:  medals table of 1986 European Athletics Championships\n",
      "Created table with 15 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 66\n",
      "Fetching data for mongolia_provinces_population_79_89_00_09\n",
      "Query:  census stats of provinces of Mongolia\n",
      "Created table with 20 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 67\n",
      "Fetching data for tulsa_shock_2010\n",
      "Query:  player stats of 2010 Tulsa Shock season\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 68\n",
      "Fetching data for london_heathrow_busiest_routes_2012\n",
      "Query:  60 busiest international routes to and from Heathrow Airport in 2012\n",
      "Created table with 60 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 69\n",
      "Fetching data for hungarian_grand_prix_qualifying_2012\n",
      "Query:  results of qualifying races of 2012 Hungarian Grand Prix\n",
      "Created table with 24 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 70\n",
      "Fetching data for farum_park_national_games\n",
      "Query:  national games played at Farum Park from 26 April 2000 to 9 June 2009\n",
      "Created table with 70 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 71\n",
      "Fetching data for india_poverty_2007\n",
      "Query:  stats on poverty in India\n",
      "Created table with 28 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 72\n",
      "Fetching data for us_president_elections_idaho_2008\n",
      "Query:  results of the United States presidential elections in Idaho, 2008\n",
      "Created table with 44 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 73\n",
      "Fetching data for blackburn_rovers_1995_1996\n",
      "Query:  results of 1995-96 Blackburn Rovers F.C. season\n",
      "Created table with 36 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 74\n",
      "Fetching data for maurice_jarr_1958_2001\n",
      "Query:  films of Maurice Jarr from 1958 to 2001\n",
      "Created table with 25 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 75\n",
      "Fetching data for 49ers_rams_rivalry_1980s\n",
      "Query:  game stats of 49ers-Rams rivalry from 1980 to 1989\n",
      "Created table with 20 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 76\n",
      "Fetching data for academy_award_best_actress_2000s\n",
      "Query:  actresses nominated for Academy Award for Best Actress in 2000s\n",
      "Created table with 48 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 77\n",
      "Fetching data for mens_walking_20km_record_1911_2007\n",
      "Query:  men's 20 kilometres walk world record progression from 1911-01-01 to 2007-12-31\n",
      "Created table with 44 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 78\n",
      "Fetching data for booknotes_interviews_1996\n",
      "Query:  Booknotes interviews first aired in 1996\n",
      "Created table with 23 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 79\n",
      "Fetching data for bafta_best_actor_leading_role_2000s\n",
      "Query:  wins and nominations of BAFTA award for best actor in a leading role from 2000 to 2009\n",
      "Created table with 50 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 80\n",
      "Fetching data for troublemaker_song_release_history\n",
      "Query:  radio and release hisory of Troublemaker (Olly Murs song)\n",
      "Created table with 21 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 81\n",
      "Fetching data for wind_power_kansas_2001_2011\n",
      "Query:  wind power stats in Kansas from 2001 to 2011\n",
      "Created table with 11 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 82\n",
      "Fetching data for fljotsdalshreppur_population_1998_2011\n",
      "Query:  population stats in Fljotsdalshreppur municipality from 1998 to 2011\n",
      "Created table with 14 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 83\n",
      "Fetching data for social_credit_party_1951_1984\n",
      "Query:  election results of Social Credit Party (New Zealand) from 1951 to 1984\n",
      "Created table with 12 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 84\n",
      "Fetching data for soekarno_hatta_airport_2001_2012\n",
      "Query:  volume stats of Soekarno-Hatta International Airport from 2001 to 2012\n",
      "Created table with 12 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 85\n",
      "Fetching data for ulrike_maier_season_standings_1985_1994\n",
      "Query:  season standings of Ulrike Maier from 1985 to 1994\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 86\n",
      "Fetching data for black_dog_barking_charts\n",
      "Query:  charts positions of the album Black Dog Barking \n",
      "Created table with 19 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 87\n",
      "Fetching data for ahl_scoring_leaders_1979_1980\n",
      "Query:  top 10 scorers of 1979-1980 AHL season\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 88\n",
      "Fetching data for boxing_medal_table_olympics_1928\n",
      "Query:  medals table for boxing at the 1928 Summer Olympics\n",
      "Created table with 9 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 89\n",
      "Fetching data for uefa_intertoto_cup_top_scorers_2001\n",
      "Query:  top scorers of 2001 UEFA Intertoto Cup\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 90\n",
      "Fetching data for montenegro_eurovision_2007\n",
      "Query:  songs and results from MontenegroSong 2007 competition\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 91\n",
      "Fetching data for great_ukrainians_series\n",
      "Query:  Great Ukrainians documentary series\n",
      "Created table with 21 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 92\n",
      "Fetching data for elland_road_leeds_united_attendance_2000_2012\n",
      "Query:  attendance stats of Leeds United at Elland Road from 2000-2001 to 2012-2013\n",
      "Created table with 13 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 93\n",
      "Fetching data for sydney_hobart_yacht_race_2007\n",
      "Query:  top 10 line honour results of 2007 Sydney to Hobart yacht race\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 94\n",
      "Fetching data for kal_penn_tv_roles_1999_2013\n",
      "Query:  television roles of Kal Penn from 1999 to 2013\n",
      "Created table with 13 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 95\n",
      "Fetching data for susen_tiedtke_achievements_1987_2000\n",
      "Query:  achievements of Susen Tiedtke from 1987 to 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table with 13 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 96\n",
      "Fetching data for bifa_british_independent_film_2010_2012\n",
      "Query:  winners and nominees of BIFA Award for Best British Independent Film from 2010 to 2012\n",
      "Created table with 15 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 97\n",
      "Fetching data for through_the_wormhole_season_4\n",
      "Query:  episodes of Through the Wormhole season 4\n",
      "Created table with 10 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 98\n",
      "Fetching data for un_habitat_scroll_of_honour_award_1991\n",
      "Query:  winners of UN-Habitat Scroll of Honour Award in 1991\n",
      "Created table with 7 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 99\n",
      "Fetching data for miss_universe_semifinal_scores_1993\n",
      "Query:  semifinal scores of Miss Universe 1993\n",
      "Created table with 6 rows\n",
      "\n",
      "====================\n",
      "\n",
      "Table # 100\n",
      "Fetching data for woodley_season_1_2012\n",
      "Query:  episodes of Woodley season 1\n",
      "Created table with 8 rows\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tg = TableGenerator_JSON()\n",
    "\n",
    "runner = ExperimentRunner(tg, metadata_path=\"DATA/Benchmark/cfg.json\")\n",
    "\n",
    "print(\"\\n====================\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"Table # %d\" % (i+1))\n",
    "    idx = \"%d\" % i\n",
    "    table = runner.fetch_data(idx)\n",
    "    print(\"\\n====================\\n\")\n",
    "    \n",
    "runner.save_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
